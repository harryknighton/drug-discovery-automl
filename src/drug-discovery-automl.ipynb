{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4264f525-b3df-46f7-89a5-dc8897b1aa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d97ec0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:No normalization for BCUT2D_MWHI\n",
      "WARNING:root:No normalization for BCUT2D_MWLOW\n",
      "WARNING:root:No normalization for BCUT2D_CHGHI\n",
      "WARNING:root:No normalization for BCUT2D_CHGLO\n",
      "WARNING:root:No normalization for BCUT2D_LOGPHI\n",
      "WARNING:root:No normalization for BCUT2D_LOGPLOW\n",
      "WARNING:root:No normalization for BCUT2D_MRHI\n",
      "WARNING:root:No normalization for BCUT2D_MRLOW\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from enum import Enum\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "from typing import Tuple, List, Type, Union\n",
    "\n",
    "import chemprop\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import pytorch_lightning as tl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem\n",
    "import scipy as sp\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, max_error\n",
    "from sklearn.model_selection import KFold\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import ReLU, Linear, MSELoss, Dropout\n",
    "from torch.nn.functional import log_softmax, relu, dropout\n",
    "from torch.optim import AdamW\n",
    "from torchmetrics.functional import pearson_corrcoef\n",
    "from torch_geometric.data import Data, Batch, InMemoryDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import (\n",
    "    Sequential,\n",
    "    MessagePassing, GCNConv, GATConv, GATv2Conv, GINConv,\n",
    "    Aggregation, global_mean_pool, global_max_pool, global_add_pool\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e47f2f66-8a12-42ec-93a0-bd78bbc25df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "def correlation_squared_score(y_true, y_pred):\n",
    "    return np.power(pearson_corrcoef(y_pred, y_true), 2)\n",
    "\n",
    "def correlation_score(y_true, y_pred):\n",
    "    return pearson_corrcoef(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1e5332-f38d-421a-aabd-66e867cac64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment_path(dataset_name, using_embeddings):\n",
    "    data_used = 'SD_DR' if using_embeddings else 'DR'\n",
    "    return f\"{dataset_name}/{data_used}\"\n",
    "\n",
    "def generate_run_name():\n",
    "    return datetime.now().strftime(\"%m-%d-%Y_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa83e9d0-6db9-420a-a532-6e133fe86df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hyper_parameters(parameters, architecture, run_dir):\n",
    "    filepath = run_dir + '/' + 'parameters.txt'\n",
    "    with open(filepath, 'w') as out:\n",
    "        out.write(str(architecture))\n",
    "        out.write('\\n')\n",
    "        out.write(str(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7327e01f-87b0-4784-8336-cc5a95c62f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_architecture(architecture, run_dir):\n",
    "    filepath = run_dir + '/' + 'architecture.pkl'\n",
    "    with open(filepath, 'wb') as out:\n",
    "        pickle.dump(architecture, out)\n",
    "        \n",
    "def load_architecture(run_dir):\n",
    "    filepath = run_dir + '/' + 'architecture.pkl'\n",
    "    with open(filepath, 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48eebc30-5977-4244-83dc-f61afd33b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_k_fold_metrics(fold_metrics, run_dir):\n",
    "    filepath = run_dir + '/' + 'results.json'\n",
    "    stacked_metrics = {name: np.array([fold[name] for fold in fold_metrics]) for name in fold_metrics[0]}\n",
    "    means = {name: float(np.mean(metrics)) for name, metrics in stacked_metrics.items()}\n",
    "    variances = {name: float(np.var(metrics)) for name, metrics in stacked_metrics.items()}\n",
    "    metrics = {name: {'mean': means[name], 'variance': variances[name]} for name in stacked_metrics}\n",
    "    with open(filepath, 'w') as out:\n",
    "        json.dump(metrics, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68250f4f-aa67-484a-bd4b-627502f8d48b",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "Each dataset has the following columns: CID, SD, SD Z-score, DR, XC50, activity, neut-smiles, num. atoms and max atomic num. \n",
    "\n",
    "For the base of the project I will only be using the DR value and the neut-smiles representation. For any compound with a non-null DR value, the activity is 'Active'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32e62abe-4572-48ef-94ac-578c0bacb15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_connectivity(mol):\n",
    "    conns = []\n",
    "    b2a = mol.b2a\n",
    "    a2b = mol.a2b\n",
    "    for aI, bonds in enumerate(a2b):\n",
    "        neighbours = [(b2a[bI], aI) for bI in bonds]\n",
    "        conns.extend(neighbours)\n",
    "    return conns\n",
    "\n",
    "def read_data(filepath: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(filepath)\n",
    "\n",
    "def process_data(df: pd.DataFrame) -> Tuple[List[Tensor]]:\n",
    "    smiles = df['neut-smiles']\n",
    "    mols = [chemprop.features.featurization.MolGraph(s) for s in smiles]\n",
    "    xs = [Tensor(m.f_atoms) for m in mols]\n",
    "    conns = [get_connectivity(m) for m in mols]\n",
    "    edge_indexes = [torch.tensor(conn, dtype=torch.long).T.contiguous() for conn in conns]\n",
    "    ys = Tensor(df['DR'].values)\n",
    "    return [Data(x=x, edge_index=index, y=y) for x, index, y in zip(xs, edge_indexes, ys)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5c20644-c7f6-49a1-9a7a-4aa62e66555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRDataset(InMemoryDataset):\n",
    "    def __init__(self, root):\n",
    "        super().__init__(root)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "        \n",
    "    def __get__(self, idx):\n",
    "        return self.get(idx)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [DATAFILE]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['processed_data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        df = read_data(self.root + '\\\\' + self.raw_file_names[0])\n",
    "        dr_df = df[df['DR'].notnull()]\n",
    "        data_list = process_data(dr_df)\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7dee003-8341-41e4-879e-a40188baaadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, ratio):\n",
    "    num_train_samples = int(ratio * len(dataset))\n",
    "    training_dataset = dataset.index_select(slice(num_train_samples))\n",
    "    validation_dataset = dataset.index_select(slice(num_train_samples, None))\n",
    "    return training_dataset, validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7fbadb6-aa5e-442d-95c4-5831fa5d490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_folds(dataset, k, seed):\n",
    "    kfold = KFold(n_splits=k, shuffle=True, random_state=seed)\n",
    "    for train_index, val_index in kfold.split(dataset):\n",
    "        train_dataset = dataset.index_select(train_index.tolist())\n",
    "        val_dataset = dataset.index_select(val_index.tolist())\n",
    "        yield train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c0e791b-b912-408b-a7c5-6ce738af72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLayer(Enum):\n",
    "    GCN = GCNConv\n",
    "    GIN = GINConv\n",
    "    GAT = GATConv\n",
    "    GATv2 = GATv2Conv\n",
    "\n",
    "class ActivationFunction(Enum):\n",
    "    ReLU = ReLU\n",
    "        \n",
    "class PoolingFunction(Enum):\n",
    "     MEAN = global_mean_pool\n",
    "     MAX = global_max_pool\n",
    "     ADD = global_add_pool\n",
    "        \n",
    "@dataclass\n",
    "class ModelArchitecture:\n",
    "    layer_types: List[GNNLayer]\n",
    "    features: List[int]\n",
    "    activation_funcs: List[ActivationFunction]\n",
    "    pool_func: PoolingFunction\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"\"\"\\\n",
    "        ModelArchitecture(\n",
    "            Layers: [{', '.join([layer.name for layer in self.layer_types])}]\n",
    "            Features: {self.features}\n",
    "            Activation Functions: [{', '.join([func.name if func is not None else 'None' for func in self.activation_funcs])}]\n",
    "            Pool Function: {self.pool_func.__name__}\n",
    "        )\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5f62568-6112-411a-ba89-1a03dabb42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_sequential_model(arch: ModelArchitecture) -> Sequential:\n",
    "    global_inputs = \"x, edge_index, batch\"\n",
    "    layers = []\n",
    "    for layer_type, num_in, num_out, activation in zip(\n",
    "        arch.layer_types,\n",
    "        arch.features[:-1],\n",
    "        arch.features[1:],\n",
    "        arch.activation_funcs\n",
    "    ):\n",
    "        layers.append((layer_type.value(num_in, num_out), \"x, edge_index -> x\"))\n",
    "        if activation is not None:\n",
    "            layers.append(activation.value(inplace=True))\n",
    "    layers.append((arch.pool_func, \"x, batch -> x\"))\n",
    "    return Sequential(global_inputs, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b144233e-bef0-46f9-8346-60e1a19cbf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitGNN(tl.LightningModule):\n",
    "    def __init__(self, architecture, metrics, batch_size):\n",
    "        super().__init__()\n",
    "        self.gnn = construct_sequential_model(architecture)\n",
    "        self.loss = MSELoss()\n",
    "        self.metrics = metrics\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def training_step(self, data, idx):\n",
    "        pred, loss = self._eval(data)\n",
    "        self._report_loss(loss, prefix='train')\n",
    "        metrics = self._calculate_metrics(data.y, pred)\n",
    "        metrics = {'train_' + name: val for name, val in metrics.items()}\n",
    "        self.log_dict(metrics, logger=True, on_step=True, batch_size=data.y.shape[0])\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, data, idx):\n",
    "        pred, loss = self._eval(data)\n",
    "        self._report_loss(loss, prefix='val')\n",
    "        metrics = self._calculate_metrics(data.y, pred)\n",
    "        self.log_dict(metrics, logger=True, on_step=False, on_epoch=True, batch_size=data.y.shape[0])\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, data, idx):\n",
    "        pred = self.gnn(data.x, data.edge_index, data.batch)\n",
    "        return data.y, pred\n",
    "    \n",
    "    def test_epoch_end(self, outputs):\n",
    "        ys, preds = [torch.concat(arr, axis=0) for arr in zip(*outputs)]\n",
    "        losses = self.loss(preds, ys.reshape(-1, 1))\n",
    "        metrics = self._calculate_metrics(ys, preds)\n",
    "        metrics['loss'] = losses\n",
    "        avg_metrics = {name: metric.mean() for name, metric in metrics.items()}\n",
    "        self.log_dict(avg_metrics, logger=False, batch_size=self.batch_size)\n",
    "        self._plot_residuals(ys, preds)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimiser = AdamW(self.gnn.parameters())\n",
    "        return optimiser\n",
    "    \n",
    "    def _eval(self, data):\n",
    "        pred = self.gnn(data.x, data.edge_index, data.batch)\n",
    "        loss = self.loss(pred, data.y.reshape(-1, 1))\n",
    "        return pred, loss\n",
    "    \n",
    "    def _report_loss(self, loss, prefix):\n",
    "        self.log('loss/' + prefix, loss, logger=True, batch_size=self.batch_size)\n",
    "        \n",
    "    def _calculate_metrics(self, y, pred):\n",
    "        local_y, local_pred = y.cpu(), pred.detach().cpu().flatten()\n",
    "        metrics = {name: metric(y_true=local_y, y_pred=local_pred) for name, metric in self.metrics.items()}\n",
    "        sanitised_metrics = {name: 0 if val == math.nan else val for name, val in metrics.items()}\n",
    "        return sanitised_metrics\n",
    "    \n",
    "    def _plot_residuals(self, ys, preds):\n",
    "        residuals = ys.reshape(-1, 1) - preds\n",
    "        plt.scatter(ys.cpu(), residuals.cpu())\n",
    "        plt.xlabel(\"Label\")\n",
    "        plt.ylabel(\"Residuals\")\n",
    "        plt.savefig(self.logger.log_dir + '/residuals.png')\n",
    "        plt.clf()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed4813fb-4b0b-4c8a-89e7-ab11af4a9ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HyperParameters:\n",
    "    random_seed: int\n",
    "    k_folds: int\n",
    "    train_test_split: float\n",
    "    batch_size: int\n",
    "    early_stop_patience: int\n",
    "    early_stop_min_delta: float\n",
    "    lr: float\n",
    "    max_epochs: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b77c74-7c8f-4076-9163-351b80740265",
   "metadata": {},
   "source": [
    "# Run trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0aba61eb-a7d9-45f0-b5ff-0bc90d3fcda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = 'AID504329'\n",
    "DATA_DIR = f'../data/{DATASET_NAME}/'\n",
    "LOG_DIR = '../logs/'\n",
    "DATAFILE = 'SD.csv'\n",
    "EXPERIMENT_DIR = LOG_DIR + generate_experiment_path(DATASET_NAME, False)\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "DEFAULT_PARAMETERS = HyperParameters(\n",
    "    random_seed=1424,\n",
    "    k_folds=4,\n",
    "    train_test_split=0.8,\n",
    "    batch_size=32,\n",
    "    early_stop_patience=10,\n",
    "    early_stop_min_delta=0.01,\n",
    "    lr=0.00003,\n",
    "    max_epochs=100\n",
    ")\n",
    "\n",
    "DEFAULT_METRICS = {\n",
    "    'mae': mean_absolute_error,\n",
    "    'rmse': root_mean_squared_error,\n",
    "    'r2': correlation_squared_score,\n",
    "    'max_error': max_error,\n",
    "    'r': correlation_score\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3992b79f-9753-4831-b5b7-613f7de98ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_architecture = ModelArchitecture(\n",
    "    layer_types=[GNNLayer.GCN, GNNLayer.GCN, GNNLayer.GCN],\n",
    "    features=[133, 64, 16, 1],\n",
    "    activation_funcs=[ActivationFunction.ReLU, ActivationFunction.ReLU, None],\n",
    "    pool_func=PoolingFunction.MEAN\n",
    ")\n",
    "\n",
    "gat_architecture = ModelArchitecture(\n",
    "    layer_types=[GNNLayer.GAT, GNNLayer.GAT, GNNLayer.GAT],\n",
    "    features=[133, 64, 16, 1],\n",
    "    activation_funcs=[ActivationFunction.ReLU, ActivationFunction.ReLU, None],\n",
    "    pool_func=PoolingFunction.MEAN\n",
    ")\n",
    "\n",
    "gatv2_architecture = ModelArchitecture(\n",
    "    layer_types=[GNNLayer.GATv2, GNNLayer.GATv2, GNNLayer.GATv2],\n",
    "    features=[133, 64, 16, 1],\n",
    "    activation_funcs=[ActivationFunction.ReLU, ActivationFunction.ReLU, None],\n",
    "    pool_func=PoolingFunction.MEAN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "866e01e7-d963-43c0-aa36-d979733b1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(architecture, params: HyperParameters):\n",
    "    tl.seed_everything(params.random_seed, workers=True)\n",
    "\n",
    "    run_dir = EXPERIMENT_DIR + '/' + generate_run_name()\n",
    "\n",
    "    dataset = DRDataset(root=DATA_DIR)\n",
    "    dataset.shuffle()\n",
    "    training_dataset, test_dataset = split_dataset(dataset, params.train_test_split)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=params.batch_size, num_workers=NUM_WORKERS)\n",
    "    fold_metrics = []\n",
    "    for train_fold, val_fold in k_folds(training_dataset, params.k_folds, params.random_seed):\n",
    "        training_dataloader = DataLoader(train_fold, batch_size=params.batch_size, shuffle=True, num_workers=NUM_WORKERS)\n",
    "        validation_dataloader = DataLoader(val_fold, batch_size=params.batch_size, num_workers=NUM_WORKERS)\n",
    "\n",
    "        model = LitGNN(architecture, DEFAULT_METRICS, params.batch_size)\n",
    "\n",
    "        checkpoint_callback = ModelCheckpoint(\n",
    "            monitor='loss/val',\n",
    "            mode='min',\n",
    "            filename='{epoch:02d}-{loss/val:.2f}',\n",
    "            save_top_k=2,\n",
    "        )\n",
    "\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor='loss/val',\n",
    "            mode='min',\n",
    "            patience=params.early_stop_patience,\n",
    "            min_delta=params.early_stop_min_delta\n",
    "        )\n",
    "\n",
    "        trainer = tl.Trainer(\n",
    "            default_root_dir=run_dir,\n",
    "            deterministic=True,\n",
    "            accelerator='gpu',\n",
    "            devices=1,\n",
    "            log_every_n_steps=1,\n",
    "            max_epochs=params.max_epochs,\n",
    "            #callbacks=[checkpoint_callback, early_stop_callback],\n",
    "            enable_progress_bar=False,\n",
    "        )\n",
    "\n",
    "        trainer.fit(\n",
    "            model,\n",
    "            training_dataloader,\n",
    "            validation_dataloader,\n",
    "        )\n",
    "\n",
    "        trainer.test(ckpt_path='best', dataloaders=test_dataloader)\n",
    "        fold_metrics.append(trainer.callback_metrics)\n",
    "\n",
    "    save_k_fold_metrics(fold_metrics, run_dir)\n",
    "    save_architecture(architecture, run_dir)\n",
    "    save_hyper_parameters(params, architecture, run_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e98a307d-694e-48ae-88b4-be1636d81f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lightning_lite.utilities.seed:Global seed set to 1424\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "WARNING:pytorch_lightning.loggers.tensorboard:Missing logger folder: ..\\logs\\AID504329\\DR\\01-10-2023_13-59-27\\lightning_logs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name | Type              | Params\n",
      "-------------------------------------------\n",
      "0 | gnn  | Sequential_ff6297 | 9.6 K \n",
      "1 | loss | MSELoss           | 0     \n",
      "-------------------------------------------\n",
      "9.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at ..\\logs\\AID504329\\DR\\01-10-2023_13-59-27\\lightning_logs\\version_0\\checkpoints\\epoch=99-step=1700.ckpt\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Loaded model weights from checkpoint at ..\\logs\\AID504329\\DR\\01-10-2023_13-59-27\\lightning_logs\\version_0\\checkpoints\\epoch=99-step=1700.ckpt\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name | Type              | Params\n",
      "-------------------------------------------\n",
      "0 | gnn  | Sequential_2b1dda | 9.6 K \n",
      "1 | loss | MSELoss           | 0     \n",
      "-------------------------------------------\n",
      "9.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at ..\\logs\\AID504329\\DR\\01-10-2023_13-59-27\\lightning_logs\\version_1\\checkpoints\\epoch=99-step=1700.ckpt\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Loaded model weights from checkpoint at ..\\logs\\AID504329\\DR\\01-10-2023_13-59-27\\lightning_logs\\version_1\\checkpoints\\epoch=99-step=1700.ckpt\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name | Type              | Params\n",
      "-------------------------------------------\n",
      "0 | gnn  | Sequential_54bb57 | 9.6 K \n",
      "1 | loss | MSELoss           | 0     \n",
      "-------------------------------------------\n",
      "9.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at ..\\logs\\AID504329\\DR\\01-10-2023_13-59-27\\lightning_logs\\version_2\\checkpoints\\epoch=99-step=1700.ckpt\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Loaded model weights from checkpoint at ..\\logs\\AID504329\\DR\\01-10-2023_13-59-27\\lightning_logs\\version_2\\checkpoints\\epoch=99-step=1700.ckpt\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.callbacks.model_summary:\n",
      "  | Name | Type              | Params\n",
      "-------------------------------------------\n",
      "0 | gnn  | Sequential_80c004 | 9.6 K \n",
      "1 | loss | MSELoss           | 0     \n",
      "-------------------------------------------\n",
      "9.6 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.6 K     Total params\n",
      "0.039     Total estimated model params size (MB)\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Restoring states from the checkpoint path at ..\\logs\\AID504329\\DR\\01-10-2023_13-59-27\\lightning_logs\\version_3\\checkpoints\\epoch=99-step=1700.ckpt\n",
      "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.utilities.rank_zero:Loaded model weights from checkpoint at ..\\logs\\AID504329\\DR\\01-10-2023_13-59-27\\lightning_logs\\version_3\\checkpoints\\epoch=99-step=1700.ckpt\n",
      "C:\\Users\\MrMil\\miniconda3\\envs\\automl-drug-discovery\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:224: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_experiment(gcn_architecture, DEFAULT_PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a415d3-c9e0-4e88-b4ca-c241ccba3877",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in [5, 546, 985]:\n",
    "    for architecture in [gcn_architecture, gat_architecture, gatv2_architecture]:\n",
    "        DEFAULT_PARAMETERS.random_seed = seed\n",
    "        run_experiment(architecture, DEFAULT_PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e63c8fb-c6de-4f3f-9086-1559b3c56401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 21916), started 0:11:17 ago. (Use '!kill 21916' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1ffd111dae6a98e8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1ffd111dae6a98e8\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir={EXPERIMENT_DIR}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f273293-ee26-4ab8-8364-67b277be7fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DRDataset(root=DATA_DIR)\n",
    "dataset.shuffle()\n",
    "training_dataset, test_dataset = split_dataset(dataset, TRAIN_TEST_SPLIT)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "\n",
    "def objective(parameters):\n",
    "    for train_fold, val_fold in k_folds(training_dataset, K_FOLDS):\n",
    "        training_dataloader = DataLoader(train_fold, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "        validation_dataloader = DataLoader(val_fold, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "        \n",
    "        model = LitGNN(architecture, DEFAULT_METRICS)\n",
    "\n",
    "        early_stop_callback = EarlyStopping(\n",
    "            monitor='loss/val',\n",
    "            mode='min',\n",
    "            patience=3,\n",
    "        )\n",
    "\n",
    "        trainer = tl.Trainer(\n",
    "            default_root_dir=run_dir,\n",
    "            deterministic=True,\n",
    "            log_every_n_steps=1,\n",
    "            max_epochs=MAX_EPOCHS,\n",
    "            callbacks=[checkpoint_callback, early_stop_callback],\n",
    "            enable_progress_bar=False\n",
    "        )\n",
    "\n",
    "        trainer.fit(\n",
    "            model,\n",
    "            training_dataloader,\n",
    "            validation_dataloader,\n",
    "        )\n",
    "        \n",
    "        return {'loss': trainer.callback_metrics['val/loss'], 'status': hp.STATUS_OK}\n",
    "\n",
    "_options = [\n",
    "    {\n",
    "        'num_layers': num_layers,\n",
    "        'layer_types': [hp.choice(str(i), GNNLayers.) i in range(num_layers)]\n",
    "        'features': [hp.choice(str(i), [layer for layer in GNNLayers]) i in range(num_layers)]\n",
    "    }\n",
    "    for num_layers in range(2, 5)\n",
    "]\n",
    "\n",
    "space = hp.choice('architecture', _options)\n",
    "\n",
    "best = hp.fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=hp.tpe.suggest,\n",
    "    max_evals=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "27774c2f-0896-4ed3-b27e-b2d71b22c221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " {\"loss\": {\"mean\": 0.1427062451839447, \"variance\": 5.5452968808822334e-05}, \"mae\": {\"mean\": 0.3190796375274658, \"variance\": 6.943024345673621e-05}, \"rmse\": {\"mean\": 0.3752123713493347, \"variance\": 9.34278141357936e-05}, \"r2\": {\"mean\": -1.9244879484176636, \"variance\": 0.022452671080827713}, \"max_error\": {\"mean\": 0.7781556844711304, \"variance\": 0.0005830924492329359}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.1427062451839447\t5.5452968808822334e-05\t0.3190796375274658\t6.943024345673621e-05\t0.3752123713493347\t9.34278141357936e-05\t-1.9244879484176636\t0.022452671080827713\t0.7781556844711304\t0.0005830924492329359\n"
     ]
    }
   ],
   "source": [
    "results=json.loads(input())\n",
    "print()\n",
    "result_arr = [str(results[name][measure]) for name in ['loss'] + list(DEFAULT_METRICS.keys()) for measure in ['mean', 'variance']]\n",
    "print(\"\\t\".join(result_arr)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
